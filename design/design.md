# 异步操作系统设计方案

* [v1]：20201013-异步操作系统设计方案.docx，由王润基起草；

## 整体目标

结合 Rust 语言的 async 无栈协程机制，从上到下重新设计一个完全异步化的操作系统。

使得系统上的 IO 密集型程序（以 Web 服务器为例）能够达到接近专用系统的高性能（高并发，低延时）。

## 背景知识

### IO 接口类型：同步与异步

 * 同步阻塞：例如 read，阻塞当前线程直到读取完成。

 * 同步非阻塞：例如 read(NONBLOCK) ，如果当前读尚未就绪则立即返回。

 * 多路复用：例如 select,poll,epoll，阻塞当前线程直到给定事件中的任何一个发生。常配合同步非阻塞接口使用。

 * 异步：例如 MPI_iread，发出读请求，返回一个 token。之后可以查询请求状态（轮询）或者用 wait 接口阻塞等待操作完成。

 * 循环队列：例如 io_uring 及硬件设备，背后有一个内核线程（或硬件上的处理器）同时处理 IO 操作，二者通过共享内存中的两个循环队列（请求队列和完成队列）传递请求状态。

### I/O接口的性能对比

上述接口的性能依次提升，易用性依次下降：

 * 同步阻塞：由于每次操作都可能阻塞当前线程，因此对于每个链接都需要创建新线程，大量线程会占用系统资源并增加线程切换开销。

 * 同步非阻塞+多路复用：只需要一个线程就可以处理全部 IO 操作。当未就绪时不会阻塞当前线程，但执行 IO 操作本身依然会占用当前线程。由于是同步的，因此每次操作请求方和执行方都不需要保存状态。

 * 异步：将 IO 操作分为提交和完成两步，可以一次进行多次提交，然后在等待完成期间执行其它计算任务。一般由后台线程完成执行，并且可以对多个请求进行调度以提高整体性能。但是由于被拆成了两步，因此双方都需要维护每个请求的状态。

 * 循环队列：异步的终极形态，具有最高的并行性和性能。生产者和消费者同时执行，不需要上下文切换。但是需要建立共享内存并防止数据竞争。

这几种方案本质上是**请求状态和执行状态**的不同组合方式：二者越松耦合，并行性越高，性能越高，但是易用性越差。

### 事件机制：中断与轮询

 * 中断：当事件发生时，当前执行流被中断，产生一个新的临时执行流，运行中断处理函数，完成后返回原执行流继续执行。例如 CPU 中断，Unix 信号。

 * 轮询：当前执行流永远不会被打断，需要主动去询问目标事件是否发生。

中断和轮询的适用场景：

 * 中断适用于事件发生不频繁，但实时性要求高的场景。中断处理过程中一般不允许嵌套中断，因此处理过程应尽量短。

 * 轮询适用于事件发生频繁，要求高并发的场景。此时若开启中断会造成大量切换开销。

### 中断分发和处理

 * 一切软件中断的根源都是 CPU 中断，它本质上是 CPU 在指令粒度上进行轮询。

 * CPU 可以根据配置将中断分发到不同的特权级上处理，但主流实现是先分发到 OS，OS 再根据情况通过信号等机制转发到用户程序。

 * RISCV 中的 N 扩展指令集实现了**用户态中断**，但目前标准还在起草阶段，模拟器和物理机都没有实现。如何利用用户态中断提高用户态驱动的性能是值得探讨的话题。

### 任务管理：进程、线程与协程

 * 进程：每个进程有独立的地址空间，因此有页表切换开销

 * 线程：每个线程有独立的栈，切换时需要保存和恢复全部寄存器

 * 协程：可以理解为状态机转移函数，执行时共用同一个栈。编译器将 async 函数变换成状态机时，函数中需要跨越 await 的变量将存放在 Future 对象中（一般在堆上），其它变量只需放在栈上或寄存器中。

#### 协程与线程的对比

无栈协程与线程（or 有栈协程）之间的对比

 * 由于协程只可能在有限的 await 语句处切换，因此需要保存的状态信息较少；而线程可能在任意指令处被中断并切换，因此需要保持整个栈空间。所以协程空间占用更少。

 * 协程切换时无需换栈，访存局部性更好。协程切换相当于一次函数返回和调用，寄存器的使用遵守调用约定 ABI，一般不会涉及全部寄存器（？）。所以协程的切换开销更小。

对于调度器而言，协程和线程没有区别，可以统一对待。事实上，在调度线程看来，执行一个线程也是一次函数调用（两次上下文切换的汇编被包装成了 switch 函数），更接近于协程。

### 异步编程模式

进化过程：

 * 原始形态：回调函数

 * 函数式封装：Promise 和 Future

 * 编译器变换：generator 和 async-await

async 语法的出现使得编写异步程序看起来就像以前的同步风格，不过它还是有一些限制：

 * async 函数具有传染性，传统函数不能调用 async 函数

 * Rust 中的 async 函数不能递归或循环调用，因为状态机不能无限嵌套下去，但是可以用 Box 绕过

Rust 的 async 机制需要运行时的配合，其中重要组件如下：

 * future：协程状态机本体

 * executor：协程的调度与执行器，相当于线程调度器

 * reactor：负责注册和触发事件回调函数，相当于中断处理函数

 * waker：唤醒协程的 token，由 future 注册给 reactor

## 关键问题

### 协程的实时性问题

无栈协程是一种非抢占式的任务调度机制，对时间敏感的任务可能无法及时调度运行。例如 GUI 任务，它大部分时间是空闲的，但当鼠标键盘事件出现时必须及时响应，否则用户会感受到卡顿。

这类事件只能通过中断方式处理，但是无栈协程只能在特定位置主动让出，不能由中断处理函数代为让出，因此就无法让紧急任务抢占运行。

可能的解决方案是保留线程机制，专门解决抢占问题。平时在 1 个线程上调度运行所有协程，当关键事件发生需要抢占时，在中断处理函数中创建一个新线程，专门执行紧急的协程，完毕后销毁新线程，回到主线程继续执行。

### 协程、线程和进程的调度

进程用于管理CPU以外的资源分配和回收；线程和协程用于CPU资源的管理（调度）；目前能实现出来的状态是，线程调度可能跨进程或在相同进程内进行，协程调度不跨线程。直观感觉，协程调度应该可以跨线程，甚至是可以跨进程。

假定协程调度可以跨线程和进程，尝试描述一下这时切换过程。

1. 同一线程内的协程切换：在rust语言中，编译器已完善搞定。协程在执行异步函数调用时，会调用编译器自动生成的有限状态机切换代码，完成如下工作。

   a.   保存当前协程的状态；（需要补充编译生成代码的分析结果）

   b.   查询就绪协程列表，按一定策略选择下一个就绪协程；

   c.    恢复下一个协程的状态，然后继续执行。

2. 同一进程内不同线程中的协程切换：在当前协程让出CPU时，如果同一线程内的协程都处于异步函数调用的等待状态中，从逻辑上应该整个线程让出CPU，选择其他线程中的就绪就绪协程来执行。这时的麻烦是，编译器生成的协程调度和OS内核实现的线程调度如何整合成一个有机整体。想像的工作过程如下。

   a.   协程调度器保存当前协程的状态；

   b.   协程调度器的代码代表当前线程，主动让出CPU；这时需要保存当前线程的状态（包括所有通用寄存器状态）；

   c.    执行线程调度器代码，查询就绪线程列表，按一定策略选择下一个就绪线程；

   d.   恢复下一个就绪线程的状态，然后继续执行协程调度器代码，按一定策略选择当前线程中的下一个就绪协程；

3. 不同进程内的协程切换：多个一个地址空间的切换；基于RISC-V的用户态中断就可以不通过内核直接切换到另一个进程。****

#### 问题讨论

1. 如果就绪协程的队列和就绪线程的队列是一个，这个选择的过程就可以统一了。

2. 如果协程切换和线程切换的可以统一，在切换时只需要依据是否在一个线程内，就可以分情况实现协程切换、协程加堆栈切换、协程加堆栈和地址空间的切换这三种切换。

3. 这几种切换的统一需要事件注册和事件处理分发机制的统一。

### 系统调用接口设计

如何设计系统调用接口？

采用vDSO可以完全支持异步系统调用；

#### vDSO的好处

1. 安全性（函数调用到vDSO的代码来访问内核服务，vDSO的代码对用户只读）；

2. 兼容性（函数调用接口保持稳定）；

3. 高效性（不用进内核的服务访问，内核与用户态的内存共享（例子：获取系统时间））

可能的做法：用rust重写vDSO的实现，然后改造rCore的系统调用接口和RustSBI的SBI接口；

如何跨越系统调用打通用户态、内核态的异步运行时？

可能的一些设计目标：                                                                                      

1. 在内核态统一进行协程调度，去掉用户态调度开销

2. 用户态和内核态分别负责状态保存（比如在 poll 的时候会进行的状态保存）

2020-10-11 交流：

1. 内核和用户态分别调度和状态保存，中间层通过 RingBuffer 进行通信

2. 低功耗：内核作为事件触发机制，尽量减少用户/内核切换，若用户没有用到内核功能则内核不占用任何 CPU 资源，与之前的 idle process 不同（低频 spin -> nothing）

没想清楚 

#### 一种可能的思路

\1.   通过共享内存来查询事件触发状态、协程就绪状态；

\2.   用户态和内核态同时支持协程调度和线程调度，内核通过vDSO提供一致的协程和线程调度器代码；

\3.   跨地址空间的切换只通过内核协调。

### 性能分析

如果采用每个用户线程对应到一个内核协程的方式，性能提升可能并不明显：

1. 对于传统的同步应用：我们还是需要给每个用户线程开一个用户栈，即使现在每个核仅需要一个中断栈（但其实 Linux 似乎已经做到了这一点？）性能提升大概体现在缓存友好、内核态两个 Future 之间切换的开销较小。对于多线程的 I/O 密集型应用似乎会有一定效果；

   // 对于同步调用，感觉不太可能有比较大的提升

2. 对于异步应用：其用户线程数很少，导致很少发生内核态 Future 切换，我们的异步内核相比传统内核应该没有明显提升。

   // 需要重写用户异步底层接口

需要注意的是，通过 epoll 降低系统调用开销，和通过 io_uring 大幅减少系统调用次数，以及类似 Rust 这样的 async 降低用户态协程切换开销，这些优化和我们的内核是同步还是异步都是没有关系的。目前看来，如果沿用以往的 Linux 系列接口，异步 OS 的性能提升就会仅仅体现在内核栈数目的（可能）减少以及内核态两个 Future 切换开销（可能）比传统意义上两个内核线程切换开销要小。

如果想实现更大的性能提升，我们确实需要设计新的一套 syscall 打通用户、内核态的运行时，这意味着完全不同的用户程序开发方式。但为了兼容性，我们希望它能够用来实现 Nginx/Redis 的统一事件机制接口。

## 实现方案

### 目标用户程序

1. 基于事件循环机制的异步应用，模型为每个用户线程为一个大事件循环，在循环开头通过 syscall 检查哪些请求已经完成，随即对它们进行处理，处理过程中可能提交新的请求，然后进入下一个循环。目前已有的例子是，Nginx 和 Redis 都将事件机制封装成统一的接口，并提供基于 select/poll/epoll/kqueue/iocp/io_uring 其中的某几种系统调用的实现。如 Redis 的 [ae_.c](https://github.com/openanolis/redis/tree/feat/io_uring/src) 以及 Nginx 的 [ngx__module.c](https://github.com/openanolis/nginx/tree/feat/io_uring/src/event/modules)。tokio 的底层 mio 应该也是基于 epoll 实现的，但目前并没有深入调研。也就是说，无论应用的代码看起来是什么样子，这种应用的核心要素在于事件循环、addEvent 和 poll。

2. 传统面向过程的同步应用，每个线程都是从头执行到尾，它并不会关心每个调用是否被阻塞，对于它来说完全是透明的。

### 初步实现方案

每个用户线程映射到内核中的一个无栈协程，也就是一个 Rust Future。随后内核跑一个优先级线程调度器调度多个内核线程，每个内核线程是一个 Executor 调度多个内核协程。目前的 zCore 是单核的，为了支持多核需要新增优先级线程调度器。

优先级体现在：当遇到了响应优先级或实时性要求较高的中断时（如键盘等），可以将对应的 Future 从低优先级线程取出放到高优先级线程，由此可以降低延迟，但是站在高性能的角度这是可以舍弃的。

我们可以先基于 zcore-linux-bare-metal 进行改进，支持 poll/epoll/io_uring 系统调用，确定一个测试平台并实现对应的块设备和网卡驱动以及 TCP/IP 协议栈，为了提高性能均支持 DMA，以中断方式访问（根据 Biscuit 论文可以用 interrupt coalescing 技术将网卡中断间隔调整到 128 微秒）。由此，可以基于 Nginx/Redis 进行简单的 benchmark。

## TODO

### 相关资料

#### io_uring

 * io_uring 文档： https://kernel.dk/io_uring.pdf

 * io_uring 相关 syscall 与用户库：https://openanolis.org/en/sig/high-perf-storage/basic/

 * io_uring 测试框架：https://github.com/OpenAnolis/perf-test-for-io_uring https://github.com/OpenAnolis/io_uring-echo-server

 * Redis 的 io_uring 实现：https://github.com/openanolis/redis/tree/feat/io_uring

 * Nginx 的 io_uring 实现：https://github.com/openanolis/nginx/tree/feat/io_uring

 * AIO 的新归宿：io_uring： https://zhuanlan.zhihu.com/p/62682475

 * 基于 io_uring 和绑核的 Rust 异步运行时： https://github.com/DataDog/scipio

 * RSoC: improving drivers and kernel (largely io_uring) https://www.redox-os.org/news/io_uring-0/  把0换为 1，2，3，4 可以看到进一步的系列文章



#### async runtime

 * 金枪鱼之夜：Build an Async Runtime for Rust from Scratch https://tuna.moe/event/2020/rust-async-runtime/

 * async collections in github https://github.com/stjepang



